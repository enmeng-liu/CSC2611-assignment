{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "rg65 = ('cord', 'smile', 'rooster', 'voyage', 'noon', 'string', 'fruit', 'furnace', 'autograph', 'shore',\n",
    "     'automobile', 'wizard', 'mound' ,'stove', 'grin', 'implement', 'asylum', 'fruit', 'asylum', 'monk',\n",
    "     'graveyard', 'madhouse', 'glass', 'magician', 'boy', 'rooster', 'cushion','jewel', 'monk', 'slave',\n",
    "     'asylum', 'cemetery', 'coast', 'forest', 'grin', 'lad', 'shore', 'woodland', 'monk', 'oracle',\n",
    "     'boy', 'sage', 'automobile', 'cushion', 'mound', 'shore', 'lad', 'wizard', 'forest', 'graveyard',\n",
    "     'food', 'rooster', 'cemetery', 'woodland', 'shore', 'voyage', 'bird', 'woodland', 'coast', 'hill',\n",
    "     'furnace', 'implement', 'crane', 'rooster', 'hill', 'woodland', 'car', 'journey', 'cemetery', 'mound',\n",
    "     'glass', 'jewel', 'magician', 'oracle', 'crane', 'implement', 'brother', 'lad', 'sage', 'wizard',\n",
    "     'oracle', 'sage', 'bird', 'crane', 'bird', 'cock', 'food', 'fruit', 'brother', 'monk',\n",
    "     'asylum', 'madhouse', 'furnace', 'stove', 'magician', 'wizard', 'hill', 'mound', 'cord', 'string',\n",
    "     'glass', 'tumbler', 'grin', 'smile', 'serf', 'slave', 'journey', 'voyage', 'autograph', 'signature',\n",
    "     'coast', 'shore', 'forest', 'woodland', 'implement', 'tool', 'cock', 'rooster', 'boy', 'lad',\n",
    "     'cushion', 'pillow', 'cemetery', 'graveyard', 'automobile', 'car', 'midday', 'noon', 'gem', 'jewel')\n",
    "human_similarity = (0.02, 0.04, 0.04, 0.05, 0.06, 0.11, 0.14, 0.18, 0.19, 0.39,\n",
    "            0.42, 0.44, 0.44, 0.45, 0.57, 0.79, 0.85, 0.88, 0.90, 0.91,\n",
    "            0.96, 0.97, 0.97, 0.99, 1.00, 1.09, 1.18, 1.22, 1.24, 1.26,\n",
    "            1.37, 1.41, 1.48, 1.55, 1.69, 1.78, 1.82, 2.37, 2.41, 2.46,\n",
    "            2.61, 2.63, 2.63, 2.69, 2.74, 3.04, 3.11, 3.21, 3.29, 3.41,\n",
    "            3.45, 3.46, 3.46, 3.58, 3.59, 3.60, 3.65, 3.66, 3.68, 3.82,\n",
    "            3.84, 3.88, 3.92, 3.94, 3.94)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Mengzelev\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract the 5000 most common English words (denoted by W) based on unigram frequencies in the Brown corpus\n",
    "fdist = nltk.FreqDist(w.lower() for w in brown.words())\n",
    "wfreq = fdist.most_common(5000)\n",
    "w = list(x[0] for x in wfreq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5031"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Update W by adding rg65 words and make sure they are at 0-len\n",
    "rg65_set = set(rg65)\n",
    "for word in rg65_set:\n",
    "    if word not in w:\n",
    "        w.insert(0, word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct bigram for the brown corpus\n",
    "from collections import Counter\n",
    "from nltk.util import ngrams\n",
    "\n",
    "bigrams = ngrams(brown.words(), 2)\n",
    "bigrams_freq = Counter(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct a word-context vector model (denoted by M1) by collecting bigram counts for words in W\n",
    "m1_list= []\n",
    "for i in range(0, len(w)):\n",
    "    row = []\n",
    "    for j in range(0, len(w)):\n",
    "        row.append(bigrams_freq[(w[i], w[j])])\n",
    "    m1_list.append(row)\n",
    "m1 = np.array(m1_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram = ngrams(brown.words(), 1)\n",
    "unigram_freq = Counter(unigram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_unigram = float(sum(fdist.values()))\n",
    "sum_bigram = float(sum(bigrams_freq.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Compute positive pointwise mutual information on M1. Denote this model as M1+\n",
    "# https://stackoverflow.com/questions/22118350/python-sentiment-analysis-using-pointwise-mutual-information\n",
    "import math\n",
    "def ppmi(w1, w2):\n",
    "    p12 = bigrams_freq[(w1, w2)] / float(sum_bigram)\n",
    "    if p12 == 0:\n",
    "        return 0\n",
    "    p1 = fdist[w1] / float(sum_unigram)\n",
    "    p2 = fdist[w2] / float(sum_unigram)\n",
    "    return max(0, math.log(p12/float(p1*p2),2))\n",
    "\n",
    "m1plus_list = []\n",
    "for i in range(0, len(w)):\n",
    "    row = []\n",
    "    for j in range(0, len(w)):\n",
    "        ans = ppmi(w[i], w[j])\n",
    "        row.append(ans)\n",
    "    m1plus_list.append(row)\n",
    "m1plus = np.array(m1plus_list)\n",
    "m1plus\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct a latent semantic model (denoted by M2) by applying PCA to M1+\n",
    "from sklearn.decomposition import PCA\n",
    "def m2_pca(dimension):\n",
    "    pca = PCA(n_components=dimension)\n",
    "    return np.array(pca.fit_transform(m1plus))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.71592548,  0.02685898,  0.66079346, ..., -0.40596109,\n",
       "         0.14647425,  0.34095178],\n",
       "       [-1.23321432, -0.54208781,  0.35063855, ..., -0.68710369,\n",
       "        -0.38029985,  0.2840805 ],\n",
       "       [-1.37749283,  0.04963463,  0.3034222 , ..., -0.98636539,\n",
       "        -0.28536521,  0.54078089],\n",
       "       ...,\n",
       "       [-1.16153642, -0.35345861, -0.89991321, ...,  0.29710302,\n",
       "         0.58288933,  0.2485601 ],\n",
       "       [-1.58068971,  1.06806476, -4.79492866, ..., -4.83227689,\n",
       "        -2.56303421,  0.91626869],\n",
       "       [-1.83436506,  0.76338435, -3.97294754, ..., -3.77890211,\n",
       "        -2.06882947,  0.48038537]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2_10 = m2_pca(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2_100 = m2_pca(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2_300 = m2_pca(300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the index for each word of rg65 in w\n",
    "rg65_id = {}\n",
    "for i in range(0, len(rg65)):\n",
    "    for j in range(0,len(w)):\n",
    "        if w[j] == rg65[i]:\n",
    "            rg65_id[rg65[i]] = j\n",
    "            break  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the Pearson correlation between cosine similarities of model and human similarities\n",
    "# from scipy.spatial import distance\n",
    "from scipy import stats\n",
    "def pearson_cos_human(model):\n",
    "    cos_sim = []\n",
    "    for i in range(0, 65):\n",
    "        cos_sim.append(distance.cosine(model[rg65_id[rg65[i+i]]], model[rg65_id[rg65[i+i+1]]]))\n",
    "    return stats.pearsonr(cos_sim, human_similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.05494857263247904, 0.6637493854323102)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearson_cos_human(m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.0030878788110033986, 0.9805237578055053)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearson_cos_human(m1plus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.1959481988489057, 0.11773792935085334)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearson_cos_human(m2_10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.2923124144401581, 0.018135929609137593)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearson_cos_human(m2_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-0.2711279136635144, 0.028919139522779563)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pearson_cos_human(m2_300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"plays\" in w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get common test case\n",
    "test = []\n",
    "with open(\"tests/word-test-v1.txt\") as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        words = line.split(\" \")\n",
    "        if len(words) == 4 and words[0] in w and words[1] in w and words[2] in w and words[3] in w:\n",
    "            test.append(line)\n",
    "\n",
    "with open(\"tests/word-test-common.txt\", mode=\"w+\") as f:\n",
    "    for line in test:\n",
    "        f.write(line + '\\n')\n",
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_id(word):\n",
    "    for i, ww in enumerate(w):\n",
    "        if ww == word:\n",
    "            return i\n",
    "    return -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import spatial\n",
    "tree = spatial.KDTree(m2_300)\n",
    "def analogical_reasoning(wa1, wa2, wb1):\n",
    "    b2 = m2_300[find_id(wb1)] - m2_300[find_id(wa1)] + m2_300[find_id(wa2)]\n",
    "    return w[tree.query(b2)[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy_test_accuracy(file):\n",
    "    correct, total = 0, 0\n",
    "    with open(file) as f:\n",
    "        for line in f:\n",
    "            total += 1\n",
    "            words = line.strip().split(\" \")\n",
    "            res = analogical_reasoning(words[0], words[1], words[2])\n",
    "            if(res == words[3]):\n",
    "                correct += 1\n",
    "    return correct/float(total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analogy_test_accuracy(\"tests\\word-test-semantic.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001004016064257028"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "analogy_test_accuracy(\"tests\\word-test-syntactic.txt\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e39bb4bc0dd09b8d636f7de561eaa609baf178136a5651a1bfee988d4f249979"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit ('csc2611': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
